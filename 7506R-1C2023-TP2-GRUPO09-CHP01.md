---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False
import pandas as pd 
import numpy as np

import seaborn as sns

from matplotlib import pyplot as plt

from collections import Counter

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score

from joblib import dump, load

from os.path import exists

import string

import nltk
stopwords_es = nltk.corpus.stopwords.words('spanish')

import re
from unicodedata import normalize

import tensorflow as tf

from tensorflow import keras
from keras.preprocessing.text import one_hot, Tokenizer
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense, SpatialDropout1D
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, TextVectorization
from keras_preprocessing.sequence import pad_sequences
```

```python
SEED=9
JOBS=-2
```

```python
reviewDfOriginal = pd.read_csv("./review_train.csv")
reviewDf = reviewDfOriginal.copy()
```

```python
review_pruebasOriginal = pd.read_csv("./review_test.csv")
review_pruebas = review_pruebasOriginal.copy()
```

```python
reviewDf = reviewDf.drop(["ID"],  axis='columns', inplace=False)
reviewDf
```

```python
reviewDf_x = reviewDf.drop(["sentimiento"],  axis='columns', inplace=False)

reviewDf_y = reviewDf['sentimiento'].copy()

x_train, x_test, y_train, y_test = train_test_split(reviewDf_x,
                                                    reviewDf_y,
                                                    test_size=0.30,
                                                    random_state=SEED,
                                                    shuffle=True
                                                    )
```

# Bayes Naive

```python
modeloBayesNaive = make_pipeline(TfidfVectorizer(), MultinomialNB())
```

```python
if not exists('submissions/TP2/naiveBayes-0.csv'):
    modeloBayesNaive.fit(x_train.review_es, y_train)
    prediccion = modeloBayesNaive.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccion})
    df_submission.to_csv('submissions/TP2/naiveBayes-0.csv', index=False)
```

# Random Forest


# XGBoost



# Red Neuronal


## Pre procesamiento inicial


Antes de tokenizar las reviews, vamos a hacer un proceso de pre procesamiento general para elimiar palabras innecesarias, como preprosiciones.

Mas adelante vamos a realizar un proceso mas exhaustivo, esto es simplemente una limpieza general

```python
if not exists('reviews_filtradas.csv'):
    frasesFiltradas = []
    for index, value in reviewDf["review_es"].items():
        #Ponemos todas las palabras en lowercase
        value = value.lower()

        #Saco las stopwords
        valueFiltrado = [x for x in value.split() if x not in stopwords_es]
        #Vuelvo a unir el texto
        valueFiltrado = " ".join(valueFiltrado)

        #Saca los diacriticos de letras como vocales, etc (la ñ se mantiene)
        #Expresion regular obtenida de: https://es.stackoverflow.com/a/139811
        valueFiltrado = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", 
                                normalize( "NFD", valueFiltrado), 0, re.I)
        valueFiltrado = normalize( 'NFC', valueFiltrado)

        #Saco los signos de puntuacion
        #Funcion obtenida de: https://stackoverflow.com/a/266162/13683575
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', string.punctuation))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¡'))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¿'))
        
        #Anadimos la frase a la lista de frases filtradas
        frasesFiltradas.append(valueFiltrado)
    reviewDfFiltrado = pd.DataFrame(data={'review_es':frasesFiltradas})
    reviewDfFiltrado.to_csv('reviews_filtradas.csv', index=False)

else:
    reviewDfFiltrado = pd.read_csv("./reviews_filtradas.csv")

```


Ahora, nuestro dataframa se ve asi:

```python
reviewDfFiltrado
```

## *TODO* ESTO NO LO USE TODAVIA,


## Creacion de los sets de entrenamiento


Transformamos la columna objetivo de "positivo" y "negativo" a 0 y 1 para poder usarlos en la red neuronal

```python
y = reviewDf['sentimiento']
y = np.array(list(map(lambda x: 1 if x=="positivo" else 0, y)))
y
```

```python
x_train, x_test, y_train, y_test = train_test_split(reviewDfFiltrado["review_es"],
                                                    y, 
                                                    test_size=0.3,  #proporcion 70/30
                                                    random_state=SEED) #Semilla 9, como el Equipo !!
```


Tensorflow.Dataset lo usamos como "wrapper" a nuestros sets de entrenamiento.

```python
text_dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
```

## TODO EXPLICAR ESTA FUNCION

```python
def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, b"<br\\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch
```

```python
vocabulary = Counter()
    
for X_batch, y_batch in text_dataset_train.batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))
```

Vamos a ver si el vocabulario se genero correctamente

```python
vocabulary.most_common()[1:5]
```

Vemos que la palabra mas frecuente es "pelicula", cosa que hace sentido.

Si no hubiesemos hecho el proceso de remocion de stopwords, es probable que la palabra mas frecuente seria una preposicion


Para reducir el tamaño del vocabulario, vamos a quedarnos solamente con las 10000 palabras mas frecuentes.

Decidimos quedarnos con las primeras 10000

```python
vocab_size = 10000
truncated_vocabulary = [
word for word, count in vocabulary.most_common()[:vocab_size]]
```

## TODO Explicar lo de la tabla

```python
words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)
```

Esta funcion TODO: PONER LO QUE HACE

```python
def encode_words(X_batch, y):
    return table.lookup(X_batch), y

train_set = text_dataset_train.batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)
```

## Creacion de la red

```python
embed_size = 128
redNeuronal = keras.models.Sequential([
    
keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
        input_shape=[None]),
    
    keras.layers.GRU(128, return_sequences=True),
    
    keras.layers.GRU(128),
    
    keras.layers.Dense(1, activation="sigmoid")
])
redNeuronal.compile(loss="binary_crossentropy", optimizer="adam",
            metrics=["accuracy"])
```

```python
redNeuronal.summary()
```

```python
if exists('modelos/TP2/redNeuronalSentimiento2.joblib') == False:
    historia_modelo=redNeuronal.fit(train_set, epochs=4)
    dump(redNeuronal, 'modelos/TP2/redNeuronalSentimiento2.joblib')
else:
    redNeuronal = load('modelos/TP2/redNeuronalSentimiento2.joblib')
```

```python
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, x_test))
test_set = test_dataset.batch(32).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)
```

```python
y_pred = redNeuronal.predict(test_set)
y_predCerteza = np.where(y_pred>0.7,1,0)
y_predCerteza
```

```python
ds_validacion=pd.DataFrame(y_predCerteza,y_test).reset_index()
ds_validacion.columns=['y_pred','y_real']

tabla=pd.crosstab(ds_validacion.y_pred, ds_validacion.y_real)
grf=sns.heatmap(tabla,annot=True, cmap = 'Blues', fmt='g')
plt.show()
```

```python
#Calculo las métricas en el conjunto de evaluación
accuracy=accuracy_score(y_test,y_predCerteza)
recall=recall_score(y_test,y_predCerteza)
f1=f1_score(y_test,y_predCerteza,)
precision=precision_score(y_test,y_predCerteza)

print("Accuracy: "+str(accuracy))
print("Recall: "+str(recall))
print("Precision: "+str(precision))
print("f1 score: "+str(f1))
```

```python
if not exists('submissions/TP2/redesNeuronales2.csv'):
    yEnEspanol =y_predCerteza
    yEnEspanol = np.array(list(map(lambda x: "positivo" if x==1 else "negativo", y_predCerteza)))
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': yEnEspanol})
    df_submission.to_csv('submissions/TP2/redesNeuronales2.csv', index=False)
```

```python

```
