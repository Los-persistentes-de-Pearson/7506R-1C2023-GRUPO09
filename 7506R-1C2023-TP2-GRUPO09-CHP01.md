---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False
import pandas as pd 
import numpy as np

import seaborn as sns

from matplotlib import pyplot as plt

from collections import Counter

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

from joblib import dump, load

from os.path import exists

import string

import nltk
stopwords_es = nltk.corpus.stopwords.words('spanish')

import re
from unicodedata import normalize

import tensorflow as tf

from tensorflow import keras
from keras.preprocessing.text import one_hot, Tokenizer
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense, SpatialDropout1D
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, TextVectorization
from keras_preprocessing.sequence import pad_sequences
```

```python
SEED=9
JOBS=-2
```

```python
reviewDfOriginal = pd.read_csv("./review_train.csv")
reviewDf = reviewDfOriginal.copy()
```

```python
review_pruebasOriginal = pd.read_csv("./review_test.csv")
review_pruebas = review_pruebasOriginal.copy()
```

```python
reviewDf = reviewDf.drop(["ID"],  axis='columns', inplace=False)
reviewDf
```

```python
reviewDf_x = reviewDf.drop(["sentimiento"],  axis='columns', inplace=False)

reviewDf_y = reviewDf['sentimiento'].copy()

x_train, x_test, y_train, y_test = train_test_split(reviewDf_x,
                                                    reviewDf_y,
                                                    test_size=0.30,
                                                    random_state=SEED,
                                                    shuffle=True
                                                    )
```

# Bayes Naive

```python
modeloBayesNaive = make_pipeline(TfidfVectorizer(), MultinomialNB())
```

```python
if not exists('submissions/TP2/naiveBayes-0.csv'):
    modeloBayesNaive.fit(x_train.review_es, y_train)
    prediccion = modeloBayesNaive.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccion})
    df_submission.to_csv('submissions/TP2/naiveBayes-0.csv', index=False)
```

# Random Forest


# XGBoost



# Red Neuronal


## Pre procesamiento


Antes de tokenizar las reviews, vamos a hacer un proceso de pre procesamiento para elimiar palabras innecesarias, como preprosiciones 

```python
if not exists('reviews_filtradas.csv'):
    frasesFiltradas = []
    for index, value in reviewDf["review_es"].items():
        #Ponemos todas las palabras en lowercase
        value = value.lower()

        #Saco las stopwords
        valueFiltrado = [x for x in value.split() if x not in stopwords_es]
        #Vuelvo a unir el texto
        valueFiltrado = " ".join(valueFiltrado)

        #Saca los diacriticos de letras como vocales, etc (la ñ se mantiene)
        #Expresion regular obtenida de: https://es.stackoverflow.com/a/139811
        valueFiltrado = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", 
                                normalize( "NFD", valueFiltrado), 0, re.I)
        valueFiltrado = normalize( 'NFC', valueFiltrado)

        #Saco los signos de puntuacion
        #Funcion obtenida de: https://stackoverflow.com/a/266162/13683575
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', string.punctuation))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¡'))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¿'))
        
        #Anadimos la frase a la lista de frases filtradas
        frasesFiltradas.append(valueFiltrado)
    reviewDfFiltrado = pd.DataFrame(data={'review_es':frasesFiltradas})
    reviewDfFiltrado.to_csv('reviews_filtradas.csv', index=False)

else:
    reviewDfFiltrado = pd.read_csv("./reviews_filtradas.csv")

reviewDfFiltrado
```

```python
text_dataset = tf.data.Dataset.from_tensor_slices(reviewDfFiltrado['review_es'])
```

## Tokenizador


Creamos el tokenizador. TextVectorization() (https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization), le asigna un valor numerico a cada una de las palabras de nuestro vocabulario

```python
max_len = 50
max_features = 5000  # Maximum vocab size.

tokenizador = TextVectorization(
    output_sequence_length=max_len,
    max_tokens=max_features,
    output_mode='int',
)
```

```python
# tokenizador.adapt(reviewDfFiltrado["review_es"])
tokenizador.adapt(text_dataset.batch(64))
```

```python
tokenizador.vocabulary_size()
```

```python
# pad_sequences(tokenizador)
```

## Creamos la red neuronal

```python
embed_dim = 128
lstm_out = 196
# max_fatures = 2000

model = tf.keras.models.Sequential()

model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
# model.add(tf.keras.Input(shape=[], dtype=tf.string))

model.add(tokenizador)

print(model.predict(["humano disfrutar"]))

model.add(Embedding(input_dim=tokenizador.vocabulary_size(), 
                    output_dim=3),
#                     input_shape=[None]
         )
#                     mask_zero=True)

print(model.predict(["humano disfrutar"]))

keras.layers.GRU(128, return_sequences=True),
# keras.layers.GRU(128),
# keras.layers.Dense(1, activation="sigmoid")

# model.add(SpatialDropout1D(0.4))

model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))

model.add(Dense(5,activation='softmax'))
# model.add(Dense(2,activation='softmax'))
model.add(Dense(1,activation='softmax'))

# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])



print(model.summary())

```

## Entrenamiento


Transformamos la columna objetivo de "positivo" y "negativo" a 0 y 1

```python
y = reviewDf['sentimiento']
y = np.array(list(map(lambda x: 1 if x=="positivo" else 0, y)))
y
```

```python
x=reviewDfFiltrado["review_es"]
```

## *TODO* ESTO NO LO USE TODAVIA,

Ya llegara

```python
x_train, x_test, y_train, y_test = train_test_split(x,
                                                    y, 
                                                    test_size=0.3,  #proporcion 70/30
                                                    random_state=SEED) #Semilla 9, como el Equipo !!
```

# LIBRO

```python
text_dataset = tf.data.Dataset.from_tensor_slices((reviewDfFiltrado['review_es'], y))
```

```python
def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, b"<br\\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch
```

```python
vocabulary = Counter()
    
for X_batch, y_batch in text_dataset.batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))
```

Vamos a ver si el vocabulario se genero correctamente

```python
vocabulary.most_common()[1:5]
```

Vemos que la palabra mas frecuente es "pelicula", cosa que hace sentido.


Vamos a quedarnos solo con algunas palabras, no todas. 
Decidimos quedarnos con las primeras 10000

```python
vocab_size = 10000
truncated_vocabulary = [
word for word, count in vocabulary.most_common()[:vocab_size]]
```

```python
words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)
```

Esta funcion TODO: PONER LO QUE HACE

```python
def encode_words(X_batch, y):
    return table.lookup(X_batch), y

train_set = text_dataset.batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)
```

```python
embed_size = 128
redNeuronal = keras.models.Sequential([
    
keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
        input_shape=[None]),
    
    keras.layers.GRU(128, return_sequences=True),
    
    keras.layers.GRU(128),
    
    keras.layers.Dense(1, activation="sigmoid")
])
redNeuronal.compile(loss="binary_crossentropy", optimizer="adam",
            metrics=["accuracy"])
```

```python
redNeuronal.summary()
```

```python
if exists('modelos/redNeuronalSentimiento2.joblib') == False:
    historia_modelo=redNeuronal.fit(train_set, epochs=4)
    dump(redNeuronal, 'modelos/redNeuronalSentimiento2.joblib')
else:
    redNeuronal = load('modelos/redNeuronalSentimiento2.joblib')
```

```python
listaZeros = []
for value in review_pruebas['review_es']:
    listaZeros.append(0)
d = {'col1': [1, 2]}
```

```python
test_dataset = tf.data.Dataset.from_tensor_slices((review_pruebas['review_es'], review_pruebas['ID']))
test_set = test_dataset.batch(32).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)
```

```python
y_pred = redNeuronal.predict(test_set)
y_pred
y_predCerteza = np.where(y_pred>0.7,1,0)
y_predCerteza
```

```python
yEnEspanol =y_predCerteza
yEnEspanol = np.array(list(map(lambda x: "positivo" if x==1 else "negativo", y_predCerteza)))
yEnEspanol
```

```python
if not exists('submissions/TP2/redesNeuronales2.csv'):
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': yEnEspanol})
    df_submission.to_csv('submissions/TP2/redesNeuronales2.csv', index=False)
```
