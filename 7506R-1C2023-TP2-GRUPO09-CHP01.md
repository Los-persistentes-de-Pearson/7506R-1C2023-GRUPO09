---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False
import pandas as pd 
import numpy as np

import seaborn as sns

from matplotlib import pyplot as plt

from collections import Counter

import sklearn
from sklearn.model_selection import train_test_split
#import sklearn as sk


#modelos y metricas
import seaborn as sns
from matplotlib import pyplot as plt
from joblib import dump, load
from os.path import exists
from sklearn.model_selection import StratifiedKFold, KFold,RandomizedSearchCV, train_test_split, cross_validate
from sklearn.metrics import confusion_matrix, classification_report , f1_score, make_scorer, precision_score, recall_score, accuracy_score,f1_score

#Xval
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score

#vectorizacion
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score

from joblib import dump, load

from os.path import exists

import string

import nltk
from sklearn.pipeline import Pipeline

import sklearn ### ESTA NO SE BORRA ???? #TODO
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

from os.path import exists

if IN_COLAB == True:
    !pip install nltk

import nltk
nltk.download('stopwords')
stopwords_es = nltk.corpus.stopwords.words('spanish')

    
    
import re
from unicodedata import normalize

import tensorflow as tf

from tensorflow import keras
from keras.preprocessing.text import one_hot, Tokenizer
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense, SpatialDropout1D
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, TextVectorization
from keras_preprocessing.sequence import pad_sequences
import string

#Random forest
from sklearn.ensemble import RandomForestClassifier 
```

Constantes

```python
# Constantes
SEED=9
JOBS=-2
```

```python
reviewDfOriginal = pd.read_csv("./review_train.csv")
reviewDf = reviewDfOriginal.copy()
```

```python
review_pruebasOriginal = pd.read_csv("./review_test.csv")
review_pruebas = review_pruebasOriginal.copy()
```

```python
reviewDf = reviewDf.drop(["ID"],  axis='columns', inplace=False)
reviewDf
```

### FILTRO REVIEWS EN INGLES


Filtramos y nos quedamos solo con las reviews en epanol para mejorar la efectividad del modelo.
Hay 3 listas hechas a mano debido a que tuvimos que tener cuidado de no incluir palabras cuya raiz exitiera en español, o fueran parte de apellidos ya que en esos casos se veian contenidas (love-lace-, hate-r, excelent-e). Al tener 3 listas con ditintos grupos, fue mas facil armar la lista final. Reconocemos que no es el mejor filtro y que es posible que hayamos eliminado unas pocas reseñas en español y hayan quedado unas pocas en ingles. Sin embargo estamos seguros de que si saca la mayoria de las reseñas en ingles.

```python
lista_palabras_comunes_en = ["have","scary","nothing","issue"]
lista_palabras_positivas_en =["interesting","amusing","intelligent","pretty","beautiful"]
lista_palabras_negativas_en =["boring","bullshit","risk","loss","poor","ugly"]
lista_completa = lista_palabras_comunes_en + lista_palabras_positivas_en + lista_palabras_negativas_en


reviewDf_espanol = reviewDf [~reviewDf["review_es"].str.contains('|'.join(lista_completa))]

reviewDf_espanol
```

Antes de tokenizar las reviews, vamos a hacer un proceso de pre procesamiento general para elimiar palabras innecesarias, como preprosiciones.

Mas adelante vamos a realizar un proceso mas exhaustivo, esto es simplemente una limpieza general

```python
if not exists('reviews_filtradas.csv'):
    frasesFiltradas = []
    for index, value in reviewDf_espanol["review_es"].items():
        #Ponemos todas las palabras en lowercase
        value = value.lower()

        #Saco las stopwords
        valueFiltrado = [x for x in value.split() if x not in stopwords_es]
        #Vuelvo a unir el texto
        valueFiltrado = " ".join(valueFiltrado)

        #Saca los diacriticos de letras como vocales, etc (la ñ se mantiene)
        #Expresion regular obtenida de: https://es.stackoverflow.com/a/139811
        valueFiltrado = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", 
                                normalize( "NFD", valueFiltrado), 0, re.I)
        valueFiltrado = normalize( 'NFC', valueFiltrado)

        #Saco los signos de puntuacion
        #Funcion obtenida de: https://stackoverflow.com/a/266162/13683575
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', string.punctuation))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¡'))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¿'))
        
        #Anadimos la frase a la lista de frases filtradas
        frasesFiltradas.append(valueFiltrado)
    reviewDfFiltrado = pd.DataFrame(data={'review_es':frasesFiltradas, 'sentimiento':reviewDf_espanol['sentimiento']})
    reviewDfFiltrado.to_csv('reviews_filtradas.csv', index=False)

else:
    reviewDfFiltrado = pd.read_csv("./reviews_filtradas.csv")
```

```python
reviewDfFiltrado
```

## Particion


Hacemos la particion  en test y train con el dataset ya filtrado

```python
reviewDf_x = reviewDf_espanol.drop(["sentimiento"],  axis='columns', inplace=False)

reviewDf_y = reviewDf_espanol['sentimiento'].copy()

x_train, x_test, y_train, y_test = train_test_split(reviewDf_x,
                                                    reviewDf_y,
                                                    test_size=0.30,
                                                    random_state=SEED,
                                                    shuffle=True
                                                    )
```

# Bayes Naive


Creamos el modelo

```python
if not exists('modelos/TP2/naiveBayes-n.joblib'):

    modeloBayesNaive = make_pipeline(TfidfVectorizer(), MultinomialNB())
    modeloBayesNaive.fit(x_train.review_es, y_train)

    dump(modeloBayesNaive, 'modelos/TP2/naiveBayes-n.joblib')

else:
    modeloBayesNaive = load('modelos/TP2/naiveBayes-n.joblib')

prediccion = modeloBayesNaive.predict(x_test.review_es)
```

Miramnos como fue en test

```python
#performance
print(classification_report(y_test,prediccion))


#Creamos la matriz de confusión
tablota=confusion_matrix(y_test, prediccion)

#Grafico la matriz de confusión
sns.heatmap(tablota,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero')
```

Hacemos la submission

```python
if not exists('submissions/TP2/naiveBayes-nuevo.csv'):
    prediccionTesteo = modeloBayesNaive.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccionTesteo})
    df_submission.to_csv('submissions/TP2/naiveBayes-nuevo.csv', index=False)
```

### Otimizacion Bayes Naive


Ahora hacemos bayes naive pero con el dataset filtrado sin Stopwords para ver si mejora el score

```python
y = reviewDfFiltrado['sentimiento']
```

```python
x=reviewDfFiltrado["review_es"]

```

```python
x_train_filtrado, x_test_filtrado, y_train_filtrado, y_test_filtrado = train_test_split(x,
                                                    y, 
                                                    test_size=0.3,  #proporcion 70/30
                                                    random_state=SEED) #Semilla 9, como el Equipo !!
```

```python
if not exists('modelos/TP2/naiveBayes-sinStop.joblib'):

    modeloBayesNaive_sin_stop = make_pipeline(TfidfVectorizer(), MultinomialNB())
    modeloBayesNaive_sin_stop.fit(x_train_filtrado, y_train_filtrado)

    dump(modeloBayesNaive_sin_stop, 'modelos/TP2/naiveBayes-sinStop.joblib')

else:
    modeloBayesNaive_sin_stop = load('modelos/TP2/naiveBayes-sinStop.joblib')

prediccion_bn_sin_stop = modeloBayesNaive_sin_stop.predict(x_test_filtrado)
```

```python
prediccion_bn_sin_stop.dim
```

```python
y_test_filtrado
```

```python
#performance
print(classification_report(y_test_filtrado,prediccion_bn_sin_stop))


#Creamos la matriz de confusión
tablota=confusion_matrix(y_test_filtrado, prediccion_bn_sin_stop)

#Grafico la matriz de confusión
sns.heatmap(tablota,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero')
```

#### Vemos que dio aun peor que dejando las stopwords!!! :( :( :( (aunque solo un poquito)


#### Sin embargo, debido a las vicisitudes y derivas (del capitalismo posmoderno y ) de la complejisima sociedad en la que vivimos hacemos una prediccion a Kaggle que podria mejorar la anterior

```python
if not exists('submissions/TP2/naiveBayes-sinStop.csv'):
    prediccion_testeo_bn_filtrado = modeloBayesNaive_sin_stop.predict(review_pruebas.review_es)
    df_submission_bn_filtrado = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccion_testeo_bn_filtrado})
    df_submission_bn_filtrado.to_csv('submissions/TP2/naiveBayes-sinStop.csv', index=False)
```

#### Al hacer la rpediccion en Kaggle obtuvimos un f1_score de 0,44659. Podemos concluir que no en todos los casos (como este, es bueno eliminar las stopwords)


# Random Forest


Creamnos el modelo con parametros arbitrarios parta obtener una prediccion inicial

```python
if exists('modelos/TP2/modeloRandomForest-espanol.joblib') == False:
    #Creamos un clasificador con hiperparámetros arbitrarios
    rfc = RandomForestClassifier(n_jobs=JOBS,
                                 criterion="entropy", 
                                 random_state=SEED, 
                                 min_samples_leaf=15,
                                 min_samples_split=40,
                                 n_estimators=40, 
                                 class_weight="balanced")
    
    modeloRandomForest = make_pipeline(TfidfVectorizer(), rfc)

    modeloRandomForest.fit(x_train.review_es, y_train)

    dump(modeloRandomForest, 'modelos/TP2/modeloRandomForest-espanol.joblib')

else:
    modeloRandomForest = load('modelos/TP2/modeloRandomForest-espanol.joblib')
```

Miramos como fue en test

```python
prediccion_rf = modeloRandomForest.predict(x_test.review_es)

#performance
print(classification_report(y_test,prediccion_rf))


#Creamos la matriz de confusión
tabla=confusion_matrix(y_test, prediccion_rf)

#Grafico la matriz de confusión
sns.heatmap(tabla,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero')  
```

Guardamos el modelo

```python
if exists('modelos/TP2/modeloRandomForest-espanol.joblib') == False:
    #Nos guardamos este modelo para poder cargarlo en todas las corridas posteriores
    dump(modeloRandomForest, 'modelos/TP2/modeloRandomForest-espanol.joblib')
else:
    modeloRandomForest = load('modelos/TP2/modeloRandomForest-espanol.joblib')
```

Hacemos la submission

```python
if not exists('submissions/TP2/randomForest-espanol.csv'):
    prediccionTesteo_rf = modeloRandomForest.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccionTesteo_rf})
    df_submission.to_csv('submissions/TP2/randomForest-espanol.csv', index=False)
```

#### Optimizacion de hiperparametros con Cros Validation


Luego de algunos Grid searh's observamos que siempre elegia los parametros de "menor valor" para sample leafs, split y estimators. No asi con entripy- Gini. Decidimos poner de "base" (menor valor) los parametrtos del rf arbbitrarios usados antes. A continuacion los resultados

```python
if exists('modelos/TP2/GsRandomForest-1.joblib') == False:

    gsrf = RandomForestClassifier(class_weight="balanced")


    modeloRandomForest_cv = Pipeline(steps= [ ('tfidfVectorizer', TfidfVectorizer() ), ('gsrf', gsrf) ] )

    param_grid = { "gsrf__criterion" : ["gini", "entropy"], 
                   "gsrf__min_samples_leaf" : [15, 20, 80], #Vamos a hacer muchas combinaciones ya que solo vamos
                   "gsrf__min_samples_split" : [40, 64, 100],#a correr este modelo 1 sola vez; ya que lo vamos a 
                   "gsrf__n_estimators": [40, 60, 70] } #guardar   

    #Probamos entrenando sólo con 1 métrica: f1_scoree

    rf_gs = GridSearchCV(estimator=modeloRandomForest_cv, param_grid=param_grid, scoring="f1", cv=5, n_jobs=JOBS) #Optimizamos f1_score
    gs_fit = rf_gs.fit(x_train.review_es, y_train)
    
    #guardamos el grid search
    dump(gs_fit, 'modelos/TP2/GsRandomForest-1.joblib')

else:
    gs_fit = load('modelos/TP2/GsRandomForest-1.joblib')

gs_fit.best_params_
```

Los mejores resultaron ser los mismos que los arbitrarios a excepcion del criterion que resulto ser Gini ekl mejor

```python
#Obtenemos el mejor modelo
mejor_modelo_rf = gs_fit.best_estimator_

#Predicción

prediccion_mejor_modelo_rf = mejor_modelo_rf.predict(x_test.review_es)
prediccion_mejor_modelo_rf
```

```python
if not exists ('modelos/TP2/mejor_modelo_rf.joblib'):
    dump(mejor_modelo_rf, "modelos/TP2/mejor_modelo_rf.joblib")
```

Vemos como le va en test

```python
#performance
print(classification_report(y_test,prediccion_mejor_modelo_rf))


#Creamos la matriz de confusión
tabla=confusion_matrix(y_test, prediccion_mejor_modelo_rf)

#Grafico la matriz de confusión
sns.heatmap(tabla,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero') 
```

practicament no vario. La unica diferencia a nivel hiperparametros es que es mejor usar como criterio Gini que Entropy. Con el resto, grid search indica que los mejores son los mismos que los arbitrarios iniciales.


### Cross Validation


Hacemos Cross validation con 5 folds

```python
kfoldcv =StratifiedKFold(n_splits=5) 
scorer_fn = make_scorer(sklearn.metrics.f1_score, pos_label='positivo' )

if not exists ('modelos/TP2/resultados_cv_randomForest'):
    resultados_rf = cross_validate(mejor_modelo_rf, x_train.review_es, y_train, cv=kfoldcv,scoring=scorer_fn,return_estimator=True)
    
    dump(resultados_rf, "modelos/TP2/resultados_cv_randomForest")
else:
    resultados_rf = load("modelos/TP2/resultados_cv_randomForest")

metricas_cv_rf = resultados_rf['test_score']
```

```python
metric_labels_CV_rf = ['F1 Score']*len(metricas_cv_rf) 
sns.set_context('talk')
sns.set_style("darkgrid")
plt.figure(figsize=(8,8))
sns.boxplot(metricas_cv_rf)
plt.title("Modelo entrenado con 5 folds")
```

Se puede ver que no hay mucha variacion en los valores obtenidos por lo cual podemos concluir que es un modelo bueno para generalizar.


### Submission Random Forest

```python
if not exists('submissions/TP2/randomForest-mejorado-2.csv'):
    prediccionTesteoMejorRf = mejor_modelo_rf.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccionTesteoMejorRf})
    df_submission.to_csv('submissions/TP2/randomForest-mejorado-2.csv', index=False)
```

La submission nos dio una muy leve mejora (0,01 ptos mejor respecto al random forest anterior). No es mucho pero es trabajo honesto 
#TODO


# XGBoost

Generamos un modelo base XGBoost que permitira observar un comportamiento global del modelo a la hora de analizar sentimientos en el texto de las reseñas 

```py
if not exists('modelos/TP2/xgb_base.joblib'):
    xgb_base = make_pipeline(TfidfVectorizer(stop_words=stopwords), XGBClassifier( random_state=0, n_estimators=100))
    xgb_base.fit(x_train.review_es, y_train)
else:
    xgb_base = joblib.load("modelos/TP2/xgb_base.job")
```

Evaluando nuestra modelo con las metricas que nos interese

```py
#performance
print(classification_report(y_test,y_pred))


#Creamos la matriz de confusión
tabla=confusion_matrix(y_test, y_pred)

#Grafico la matriz de confusión
sns.heatmap(tabla,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero')
```

Generamos una primera predicción para kaggle

```py
if not exists('submissions/xgboost_base.csv'):
    pred = xgb_base.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': pred})
    df_sumission['sentimiento'] = df_sumission['sentimiento'].map({0: 'positivo', 1: 'negativo'})
    df_submission.to_csv('xgboost_base.csv', index=False)
```

### Busqueda de hiperparametros del XGBoost 

```py

if not exists('modelos/TP2/hipers_xgb.joblib'):
    estimadores = [90, 100, 110, 150]
    profundidad_max = [7, 8, 9, 10, 15]
    learning_rate = [0.01, 0.05, 0.1, 0.2]
    metrica = make_scorer(sklearn.metrics.f1_score)

    parametros = {
        'tfidfvectorizer__stop_words': [None, 'english', stopwords],
        'xgbclassifier__n_estimators': estimadores,
        'xgbclassifier__max_depth': profundidad_max,
        'xgbclassifier__learning_rate': learning_rate
    }

    modelo = make_pipeline(TfidfVectorizer(), XGBClassifier())

    modelo_rcv = RandomizedSearchCV(modelo, parametros, cv=5, scoring = metrica)
    modelo_rcv.fit(x_train.review_es, y_train)
    dump(modelo_rcv, 'modelos/TP2/hipers_xgb.joblib')
else: 
    modelo_rcv = load('modelos/TP2/hipers_xgb.joblib')
```

```py
print(modelo_rcv.best_params_)
print(modelo_rcv.best_estimator_)
print(modelo_rcv.best_score_)
```

Entrenamos un nuevo modelo usando los hiperparametros encontrados en la busqueda anterior 

```py 
if not exists('modelos/TP2/xgb_tuneado.joblib'):
    xgb_tuneado = make_pipeline(TfidfVectorizer(stop_words='english'), XGBClassifier(n_estimators = 150, max_depth = 9, learning_rate = 0.2))
    xgb_tuneado.fit(x_train.review_es, y_train)
else:
    xgb_tuneado = load('modelos/TP2/xgb_tuneado.joblib')
```

Realizo validacion cruzada del modelo para observar su comportamiento *nota* TARDA MUCHISIMO

```py
if not exists('modelos/TP2/xgb_tuneado.joblib'):
    kfoldcv =StratifiedKFold(n_splits=10)
    resultados_xgb = cross_validate(xgb_tuneado, x_train.review_es, y_train, cv=kfoldcv,scoring=metrica ,return_estimator=True)
    metricas_xgb = resultados_xgb['test_score']
    xgb_tuneado = resultados_xgb['estimator'][np.where(metricas_xgb==max(metricas_xgb))[0][0]]
```

```py
metric_labelsCV = ['F1 Score']*len(metricas_xgb)
sns.set_context('talk')
sns.set_style("darkgrid")
plt.figure(figsize=(6,5))
sns.boxplot(metricas_xgb)
plt.title("Modelo entrenado con 10 folds")
```

# Red Neuronal


## Pre procesamiento inicial




Ahora, nuestro dataframa se ve asi:

```python
reviewDfFiltrado
```

## *TODO* ESTO NO LO USE TODAVIA,


## Creacion de los sets de entrenamiento


Transformamos la columna objetivo de "positivo" y "negativo" a 0 y 1 para poder usarlos en la red neuronal

```python
y = reviewDf['sentimiento']
y = np.array(list(map(lambda x: 1 if x=="positivo" else 0, y)))
y
```

```python
x_train, x_test, y_train, y_test = train_test_split(reviewDfFiltrado["review_es"],
                                                    y, 
                                                    test_size=0.3,  #proporcion 70/30
                                                    random_state=SEED) #Semilla 9, como el Equipo !!
```


Tensorflow.Dataset lo usamos como "wrapper" a nuestros sets de entrenamiento.

```python
text_dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
```

## TODO EXPLICAR ESTA FUNCION

```python
def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, b"<br\\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch

```

```python
vocabulary = Counter()
    
for X_batch, y_batch in text_dataset_train.batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))
```

Vamos a ver si el vocabulario se genero correctamente

```python
vocabulary.most_common()[1:5]
```

Vemos que la palabra mas frecuente es "pelicula", cosa que hace sentido.

Si no hubiesemos hecho el proceso de remocion de stopwords, es probable que la palabra mas frecuente seria una preposicion


Para reducir el tamaño del vocabulario, vamos a quedarnos solamente con las 10000 palabras mas frecuentes.

Decidimos quedarnos con las primeras 10000

```python
vocab_size = 10000
truncated_vocabulary = [
word for word, count in vocabulary.most_common()[:vocab_size]]
```

## TODO Explicar lo de la tabla

```python
words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)
```

Esta funcion TODO: PONER LO QUE HACE

```python
def encode_words(X_batch, y):
    return table.lookup(X_batch), y

train_set = text_dataset_train.batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)
```

## Creacion de la red

```python
embed_size = 128
redNeuronal = keras.models.Sequential([
    
keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
        input_shape=[None]),
    
    keras.layers.GRU(128, return_sequences=True),
    
    keras.layers.GRU(128),
    
    keras.layers.Dense(1, activation="sigmoid")
])
redNeuronal.compile(loss="binary_crossentropy", optimizer="adam",
            metrics=["accuracy"])
```

```python
redNeuronal.summary()
```

```python
if exists('modelos/TP2/redNeuronalSentimiento2.joblib') == False:
    historia_modelo=redNeuronal.fit(train_set, epochs=4)
    dump(redNeuronal, 'modelos/TP2/redNeuronalSentimiento2.joblib')
else:
    redNeuronal = load('modelos/TP2/redNeuronalSentimiento2.joblib')
```

```python
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, x_test))
test_set = test_dataset.batch(32).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)
```

```python
y_pred = redNeuronal.predict(test_set)
y_predCerteza = np.where(y_pred>0.7,1,0)
y_predCerteza
```

```python
ds_validacion=pd.DataFrame(y_predCerteza,y_test).reset_index()
ds_validacion.columns=['y_pred','y_real']

tabla=pd.crosstab(ds_validacion.y_pred, ds_validacion.y_real)
grf=sns.heatmap(tabla,annot=True, cmap = 'Blues', fmt='g')
plt.show()
```

```python
#Calculo las métricas en el conjunto de evaluación
accuracy=accuracy_score(y_test,y_predCerteza)
recall=recall_score(y_test,y_predCerteza)
f1=f1_score(y_test,y_predCerteza,)
precision=precision_score(y_test,y_predCerteza)

print("Accuracy: "+str(accuracy))
print("Recall: "+str(recall))
print("Precision: "+str(precision))
print("f1 score: "+str(f1))
```

```python
if not exists('submissions/TP2/redesNeuronales2.csv'):
    yEnEspanol =y_predCerteza
    yEnEspanol = np.array(list(map(lambda x: "positivo" if x==1 else "negativo", y_predCerteza)))
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': yEnEspanol})
    df_submission.to_csv('submissions/TP2/redesNeuronales2.csv', index=False)
```

```python

```
# Ensamble de 3 modelos
