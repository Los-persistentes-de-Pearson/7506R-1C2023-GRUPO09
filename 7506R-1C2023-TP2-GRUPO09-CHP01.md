---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False
import pandas as pd 
import numpy as np

import sklearn as sk


#modelos y metricas
import seaborn as sns
from matplotlib import pyplot as plt
from joblib import dump, load
from os.path import exists
from sklearn.model_selection import StratifiedKFold, KFold,RandomizedSearchCV, train_test_split, cross_validate
from sklearn.metrics import confusion_matrix, classification_report , f1_score, make_scorer, precision_score, recall_score, accuracy_score,f1_score

#Xval
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score

#vectorizacion
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline

import sklearn ### ESTA NO SE BORRA ???? #TODO
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

from os.path import exists

import nltk



#Random forest
from sklearn.ensemble import RandomForestClassifier 

```

Constantes

```python
# Constantes
JOBS=-2
SEED=9
```

```python
reviewDfOriginal = pd.read_csv("./review_train.csv")
reviewDf = reviewDfOriginal.copy()
```

```python
review_pruebasOriginal = pd.read_csv("./review_test.csv")
review_pruebas = review_pruebasOriginal.copy()
```

```python
reviewDf = reviewDf.drop(["ID"],  axis='columns', inplace=False)
reviewDf
```

```python
reviewDf_x = reviewDf.drop(["sentimiento"],  axis='columns', inplace=False)

reviewDf_y = reviewDf['sentimiento'].copy()

x_train, x_test, y_train, y_test = train_test_split(reviewDf_x,
                                                    reviewDf_y,
                                                    test_size=0.30,
                                                    random_state=SEED,
                                                    shuffle=True
                                                    )
```

# Bayes Naive


Creamos el modelo

```python
if not exists('modelos/TP2/naiveBayes-0.joblib'):

    modeloBayesNaive = make_pipeline(TfidfVectorizer(), MultinomialNB())
    modeloBayesNaive.fit(x_train.review_es, y_train)

    dump(modeloBayesNaive, 'modelos/TP2/naiveBayes-0.joblib')

else:
    modeloBayesNaive = load('modelos/TP2/naiveBayes-0.joblib')

prediccion = modeloBayesNaive.predict(x_test.review_es)
```

Miramnos como fue en test

```python
#performance
print(classification_report(y_test,prediccion))


#Creamos la matriz de confusión
tablota=confusion_matrix(y_test, prediccion)

#Grafico la matriz de confusión
sns.heatmap(tablota,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero')
```

Hacemos la submission

```python
if not exists('submissions/TP2/naiveBayes-0.csv'):
    prediccionTesteo = modeloBayesNaive.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccionTesteo})
    df_submission.to_csv('submissions/TP2/naiveBayes-0.csv', index=False)
```

### Otimizacion Bayes Naive

```python
#TODO
```

```python
###### FALTA
```

# Random Forest


Creamnos el modelo con parametros arbitrarios parta obtener una prediccion inicial

```python
if exists('modelos/TP2/modeloRandomForest-0.joblib') == False:
    #Creamos un clasificador con hiperparámetros arbitrarios
    rfc = RandomForestClassifier(n_jobs=JOBS,
                                 criterion="entropy", 
                                 random_state=SEED, 
                                 min_samples_leaf=15,
                                 min_samples_split=40,
                                 n_estimators=40, 
                                 class_weight="balanced")
    
    modeloRandomForest = make_pipeline(TfidfVectorizer(), rfc)

    modeloRandomForest.fit(x_train.review_es, y_train)

    dump(modeloRandomForest, 'modelos/TP2/modeloRandomForest-0.joblib')

else:
    modeloRandomForest = load('modelos/TP2/modeloRandomForest-0.joblib')
```

Miramos como fue en test

```python
prediccion_rf = modeloRandomForest.predict(x_test.review_es)

#performance
print(classification_report(y_test,prediccion_rf))


#Creamos la matriz de confusión
tabla=confusion_matrix(y_test, prediccion_rf)

#Grafico la matriz de confusión
sns.heatmap(tabla,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero')  
```

Guardamos el modelo

```python
if exists('modelos/TP2/modeloRandomForest-0.joblib') == False:
    #Nos guardamos este modelo para poder cargarlo en todas las corridas posteriores
    dump(modeloRandomForest, 'modelos/TP2/modeloRandomForest-0.joblib')
else:
    modeloRandomForest = load('modelos/TP2/modeloRandomForest-0.joblib')
```

Hacemos la submission

```python
if not exists('submissions/TP2/randomForest-0.csv'):
    prediccionTesteo = modeloRandomForest.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccionTesteo})
    df_submission.to_csv('submissions/TP2/randomForest-0.csv', index=False)
```

#### Optimizacion de hiperparametros con Cros Validation


Luego de algunos Grid searh's observamos que siempre elegia los parametros de "menor valor" para sample leafs, split y estimators. No asi con entripy- Gini. Decidimos poner de "base" (menor valor) los parametrtos del rf arbbitrarios usados antes. A continuacion los resultados

```python
if exists('modelos/TP2/GsRandomForest-1.joblib') == False:

    gsrf = RandomForestClassifier(class_weight="balanced")


    modeloRandomForest_cv = Pipeline(steps= [ ('tfidfVectorizer', TfidfVectorizer() ), ('gsrf', gsrf) ] )

    param_grid = { "gsrf__criterion" : ["gini", "entropy"], 
                   "gsrf__min_samples_leaf" : [15, 20, 80], #Vamos a hacer muchas combinaciones ya que solo vamos
                   "gsrf__min_samples_split" : [40, 64, 100],#a correr este modelo 1 sola vez; ya que lo vamos a 
                   "gsrf__n_estimators": [40, 60, 70] } #guardar   

    #Probamos entrenando sólo con 1 métrica: f1_scoree

    rf_gs = GridSearchCV(estimator=modeloRandomForest_cv, param_grid=param_grid, scoring="f1", cv=5, n_jobs=JOBS) #Optimizamos f1_score
    gs_fit = rf_gs.fit(x_train.review_es, y_train)
    
    #guardamos el grid search
    dump(gs_fit, 'modelos/TP2/GsRandomForest-1.joblib')

else:
    gs_fit = load('modelos/TP2/GsRandomForest-1.joblib')

gs_fit.best_params_
```

Los mejores resultaron ser los mismos que los arbitrarios a excepcion del criterion que resulto ser Gini ekl mejor

```python
#Obtenemos el mejor modelo
mejor_modelo_rf = gs_fit.best_estimator_

#Predicción

prediccion_mejor_modelo_rf = mejor_modelo_rf.predict(x_test.review_es)
prediccion_mejor_modelo_rf
```

```python
if not exists ('modelos/TP2/mejor_modelo_rf.joblib'):
    dump(mejor_modelo_rf, "modelos/TP2/mejor_modelo_rf.joblib")
```

Vemos como le va en test

```python
#performance
print(classification_report(y_test,prediccion_mejor_modelo_rf))


#Creamos la matriz de confusión
tabla=confusion_matrix(y_test, prediccion_mejor_modelo_rf)

#Grafico la matriz de confusión
sns.heatmap(tabla,cmap='GnBu',annot=True,fmt='g')
plt.xlabel('Predicho')
plt.ylabel('Verdadero') 
```

practicament no vario. La unica diferencia a nivel hiperparametros es que es mejor usar como criterio Gini que Entropy. Con el resto, grid search indica que los mejores son los mismos que los arbitrarios iniciales.


### Cross Validation


Hacemos Cross validation con 5 folds

```python
kfoldcv =StratifiedKFold(n_splits=5) 
scorer_fn = make_scorer(sk.metrics.f1_score, pos_label='positivo' )

if not exists ('modelos/TP2/resultados_cv_randomForest'):
    resultados_rf = cross_validate(mejor_modelo_rf, x_train.review_es, y_train, cv=kfoldcv,scoring=scorer_fn,return_estimator=True)
    
    dump(resultados_rf, "modelos/TP2/resultados_cv_randomForest")
else:
    resultados_rf = load("modelos/TP2/resultados_cv_randomForest")

metricas_cv_rf = resultados_rf['test_score']
```

```python
metric_labels_CV_rf = ['F1 Score']*len(metricas_cv_rf) 
sns.set_context('talk')
sns.set_style("darkgrid")
plt.figure(figsize=(8,8))
sns.boxplot(metricas_cv_rf)
plt.title("Modelo entrenado con 5 folds")
```

Se puede ver que no hay mucha variacion en los valores obtenidos por lo cual podemos concluir que es un modelo bueno para generalizar.


### Submission Random Forest

```python
if not exists('submissions/TP2/randomForest-mejorado-2.csv'):
    prediccionTesteoMejorRf = mejor_modelo_rf.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccionTesteoMejorRf})
    df_submission.to_csv('submissions/TP2/randomForest-mejorado-2.csv', index=False)
```

La submission nos dio una muy leve mejora (0,01 ptos mejor respecto al random forest anterior). No es mucho pero es trabajo honesto 
#TODO


# XGBoost



# Red Neuronal


# Ensamble de 3 modelos
