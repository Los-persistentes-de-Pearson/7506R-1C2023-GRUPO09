---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False
import pandas as pd 
import numpy as np

import seaborn as sns

from matplotlib import pyplot as plt

from collections import Counter

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

from joblib import dump, load

from os.path import exists

import string

import nltk
stopwords_es = nltk.corpus.stopwords.words('spanish')

import re
from unicodedata import normalize

import tensorflow as tf

from tensorflow import keras
from keras.preprocessing.text import one_hot, Tokenizer
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense, SpatialDropout1D
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, TextVectorization
from keras_preprocessing.sequence import pad_sequences
```

```python
SEED=9
JOBS=-2
```

```python
reviewDfOriginal = pd.read_csv("./review_train.csv")
reviewDf = reviewDfOriginal.copy()
```

```python
review_pruebasOriginal = pd.read_csv("./review_test.csv")
review_pruebas = review_pruebasOriginal.copy()
```

```python
reviewDf = reviewDf.drop(["ID"],  axis='columns', inplace=False)
reviewDf
```

```python
reviewDf_x = reviewDf.drop(["sentimiento"],  axis='columns', inplace=False)

reviewDf_y = reviewDf['sentimiento'].copy()

x_train, x_test, y_train, y_test = train_test_split(reviewDf_x,
                                                    reviewDf_y,
                                                    test_size=0.30,
                                                    random_state=SEED,
                                                    shuffle=True
                                                    )
```

# Bayes Naive

```python
modeloBayesNaive = make_pipeline(TfidfVectorizer(), MultinomialNB())
```

```python
if not exists('submissions/TP2/naiveBayes-0.csv'):
    modeloBayesNaive.fit(x_train.review_es, y_train)
    prediccion = modeloBayesNaive.predict(review_pruebas.review_es)
    df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccion})
    df_submission.to_csv('submissions/naiveBayes-0.csv', index=False)
```

```python
prediccion
# review_pruebasOriginal['ID']
```

# Random Forest


# XGBoost



# Red Neuronal


## Pre procesamiento


Antes de tokenizar las reviews, vamos a hacer un proceso de pre procesamiento para elimiar palabras innecesarias, como preprosiciones 

```python
if not exists('reviews_filtradas.csv'):
    frasesFiltradas = []
    for index, value in reviewDf["review_es"].items():
        #Ponemos todas las palabras en lowercase
        value = value.lower()

        #Saco las stopwords
        valueFiltrado = [x for x in value.split() if x not in stopwords_es]
        #Vuelvo a unir el texto
        valueFiltrado = " ".join(valueFiltrado)

        #Saca los diacriticos de letras como vocales, etc (la ñ se mantiene)
        #Expresion regular obtenida de: https://es.stackoverflow.com/a/139811
        valueFiltrado = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", 
                                normalize( "NFD", valueFiltrado), 0, re.I)
        valueFiltrado = normalize( 'NFC', valueFiltrado)

        #Saco los signos de puntuacion
        #Funcion obtenida de: https://stackoverflow.com/a/266162/13683575
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', string.punctuation))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¡'))
        valueFiltrado =  valueFiltrado.translate(str.maketrans('', '', '¿'))
        
        #Anadimos la frase a la lista de frases filtradas
        frasesFiltradas.append(valueFiltrado)
    reviewDfFiltrado = pd.DataFrame(data={'review_es':frasesFiltradas})
    reviewDfFiltrado.to_csv('reviews_filtradas.csv', index=False)

else:
    reviewDfFiltrado = pd.read_csv("./reviews_filtradas.csv")

reviewDfFiltrado
```

```python
text_dataset = tf.data.Dataset.from_tensor_slices(reviewDfFiltrado['review_es'])
```

## Tokenizador


Creamos el tokenizador. TextVectorization() (https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization), le asigna un valor numerico a cada una de las palabras de nuestro vocabulario

```python
max_len = 50
max_features = 5000  # Maximum vocab size.

tokenizador = TextVectorization(
    output_sequence_length=max_len,
    max_tokens=max_features,
    output_mode='int',
)
```

```python
# tokenizador.adapt(reviewDfFiltrado["review_es"])
tokenizador.adapt(text_dataset.batch(64))
```

```python
tokenizador.vocabulary_size()
```

```python
# pad_sequences(tokenizador)
```

## Creamos la red neuronal

```python
embed_dim = 128
lstm_out = 196
# max_fatures = 2000

model = tf.keras.models.Sequential()

model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
# model.add(tf.keras.Input(shape=[], dtype=tf.string))

model.add(tokenizador)

print(model.predict(["humano disfrutar"]))

model.add(Embedding(input_dim=tokenizador.vocabulary_size(), 
                    output_dim=3),
#                     input_shape=[None]
         )
#                     mask_zero=True)

print(model.predict(["humano disfrutar"]))

keras.layers.GRU(128, return_sequences=True),
# keras.layers.GRU(128),
# keras.layers.Dense(1, activation="sigmoid")

# model.add(SpatialDropout1D(0.4))

model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))

model.add(Dense(5,activation='softmax'))
# model.add(Dense(2,activation='softmax'))
model.add(Dense(1,activation='softmax'))

# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])



print(model.summary())

```

## Entrenamiento


Transformamos la columna objetivo de "positivo" y "negativo" a 0 y 1

```python
y = reviewDf['sentimiento']
y = np.array(list(map(lambda x: 1 if x=="positivo" else 0, y)))
y
```

```python
x=reviewDfFiltrado["review_es"]
```

```python
x_train, x_test, y_train, y_test = train_test_split(x,
                                                    y, 
                                                    test_size=0.3,  #proporcion 70/30
                                                    random_state=SEED) #Semilla 9, como el Equipo !!
```

```python
# historia_modelo=model.fit(x_train, y_train, epochs=7, batch_size=5)
# historia_modelo=model.fit(x_train, y_train, epochs=4)
```

```python
# y_pred = model.predict(x_test)
# y_pred
```

```python
# y_predCerteza = np.where(y_pred>0.7,1,0)
# y_predCerteza
```

```python
# ds_validacion=pd.DataFrame(y_predCerteza,y_test).reset_index()
# ds_validacion.columns=['y_pred','y_real']
```

```python
# tabla=pd.crosstab(ds_validacion.y_pred, ds_validacion.y_real)
# grf=sns.heatmap(tabla,annot=True, cmap = 'Blues', fmt='g')
# plt.show()
```

# LIBRO

```python
type(reviewDfFiltrado['review_es'])
```

```python
type(y)
```

```python
text_dataset = tf.data.Dataset.from_tensor_slices((reviewDfFiltrado['review_es'], y))
```

```python
def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, b"<br\\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch
```

```python
vocabulary = Counter()
# for review in reviewDfFiltrado['review_es']:
#     vocabulary.update(review.split())
    
for X_batch, y_batch in text_dataset.batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))
```

```python
vocabulary.most_common()[:3]
```

Vemos que la palabra mas frecuente es "pelicula", cosa que hace sentido.
Es altamente probable que si no hubiesemos quitado los articulos, la palabra mas frecuente seria uno de ellos.




```python
vocab_size = 10000
truncated_vocabulary = [
word for word, count in vocabulary.most_common()[:vocab_size]]
```

```python
words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)
```

Chequeamos

```python
# fraseAChequear= b"pelicula es buena"
```

```python
# fraseAChequear
```

```python
# palabra = table.lookup(tf.constant([fraseAChequear.split()]))
# palabra
```

```python
# palabraReconstruida=""
# for i in palabra.numpy():
#     for j in i:
#         palabraReconstruida+=str(truncated_vocabulary[j]) + " "
# palabraReconstruida = palabraReconstruida[:-1]
# palabraReconstruida = bytes(palabraReconstruida, 'utf-8') 
```

```python
# palabraReconstruida
```

```python
# assert palabraReconstruida == fraseAChequear
```

Si no hay error de tipo "Assertion Error" significa que, en efecto, son lo mismo

```python
def encode_words(X_batch, y):
    return table.lookup(X_batch), y

# train_set = datasets["train"].batch(32).map(preprocess)


# x = text_dataset.map(encode_words).prefetch(1)
train_set = text_dataset.batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)



```

```python
train_set
```

```python
embed_size = 128
model1 = keras.models.Sequential([
    
keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
        input_shape=[None]),
#     print(model.predict(["humano disfrutar"]))

#     model.add(Embedding(input_dim=tokenizador.vocabulary_size(), 
#                     output_dim=3),
# #                     input_shape=[None]
#          )
    
    keras.layers.GRU(128, return_sequences=True),
    keras.layers.GRU(128),
    keras.layers.Dense(1, activation="sigmoid")
])
model1.compile(loss="binary_crossentropy", optimizer="adam",
            metrics=["accuracy"])
# history = model.fit(train_set, epochs=5)
```

```python
%tensorboard --logdir lRogs/fit

```

```python
print(model1.summary())

```

```python
# i = 0
# for y in x:
#     i+=1
#     if i == 4:
#         break
#     print(y[0])
```

```python
# historia_modelo=model1.fit(train_set, epochs=4)
```

```python
if exists('modelos/redNeuronalSentimiento2.joblib') == False:
    historia_modelo=model1.fit(train_set, epochs=4)
    dump(model1, 'modelos/redNeuronalSentimiento2.joblib')
else:
    model1 = load('modelos/redNeuronalSentimiento2.joblib')


```

```python
listaZeros = []
for value in review_pruebas['review_es']:
    listaZeros.append(0)
d = {'col1': [1, 2]}
```

```python
text_dataset2 = tf.data.Dataset.from_tensor_slices((review_pruebas['review_es'], review_pruebas['ID']))
test_set = text_dataset2.batch(32).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)
```

```python
for x in text_dataset:
    print(x)
    break
```

```python
y_pred = model1.predict(test_set)
y_pred
```

```python
y_predCerteza = np.where(y_pred>0.7,1,0)
len(y_predCerteza)
```

```python
y_predCerteza
```

```python
yEnEspanol =y_predCerteza
yEnEspanol = np.array(list(map(lambda x: "positivo" if x==1 else "negativo", y_predCerteza)))
yEnEspanol.shape
```

```python
prediccion
```

```python
len(review_pruebasOriginal['ID'])
```

```python
review_pruebas
# df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 
#                               'sentimiento': y_predCerteza})
df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccion})

# df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 
#                               'sentimiento': prediccion})

#                                   'is_canceled': df_resultados_pred["resultado"]})
```

```python
df_submission
```

```python
#     modeloBayesNaive.fit(x_train.review_es, y_train)
#     prediccion = modeloBayesNaive.predict(review_pruebas.review_es)
#     df_submission = pd.DataFrame({'id': review_pruebasOriginal['ID'], 'sentimiento': prediccion})
#     df_submission.to_csv('submissions/naiveBayes-0.csv', index=False)

# if not exists('submissions/red_neuronal_basica.csv'):
#     y_pred_testeo = modelo_hotels_1.predict(hotelsdf_testeo_filtrado)
#     y_pred_testeo_cat = np.where(y_pred_testeo>0.70,1,0)
#     df_resultados_pred = pd.DataFrame.from_records(y_pred_testeo_cat,columns = ["resultado"])
#     df_submission = pd.DataFrame({'id': hotelsdf_pruebasOriginal['id'], 'is_canceled': df_resultados_pred["resultado"]})
#     df_submission.to_csv('submissions/red_neuronal_basica.csv', index=False)


```
